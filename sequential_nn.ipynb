{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from thermography_dataset_one_layer import ThermDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'lr':0.01,\n",
    "        'epochs':1000,\n",
    "        'noise':0.001,\n",
    "        'train size':0.7,\n",
    "        'spec scale':10**12\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_tensor(df):\n",
    "    return torch.tensor(df.values, dtype=torch.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('combined_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,11:]\n",
    "y = df.iloc[:,:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.apply(lambda x: x*args['spec scale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=args['train size'], random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.000005</th>\n",
       "      <th>0.000005</th>\n",
       "      <th>0.000005</th>\n",
       "      <th>0.000005</th>\n",
       "      <th>0.000005</th>\n",
       "      <th>0.000005</th>\n",
       "      <th>0.000005</th>\n",
       "      <th>0.000005</th>\n",
       "      <th>0.000005</th>\n",
       "      <th>0.000005</th>\n",
       "      <th>...</th>\n",
       "      <th>0.000008</th>\n",
       "      <th>0.000008</th>\n",
       "      <th>0.000008</th>\n",
       "      <th>0.000008</th>\n",
       "      <th>0.000008</th>\n",
       "      <th>0.000008</th>\n",
       "      <th>0.000008</th>\n",
       "      <th>0.000008</th>\n",
       "      <th>0.000008</th>\n",
       "      <th>0.000008</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>0.532133</td>\n",
       "      <td>0.638418</td>\n",
       "      <td>0.726998</td>\n",
       "      <td>0.802254</td>\n",
       "      <td>0.866701</td>\n",
       "      <td>0.933640</td>\n",
       "      <td>1.000501</td>\n",
       "      <td>1.067615</td>\n",
       "      <td>1.134905</td>\n",
       "      <td>1.202684</td>\n",
       "      <td>...</td>\n",
       "      <td>5.303224</td>\n",
       "      <td>5.372908</td>\n",
       "      <td>5.430458</td>\n",
       "      <td>5.471816</td>\n",
       "      <td>5.494759</td>\n",
       "      <td>5.486708</td>\n",
       "      <td>5.423339</td>\n",
       "      <td>5.281082</td>\n",
       "      <td>5.076630</td>\n",
       "      <td>4.888354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>0.414041</td>\n",
       "      <td>0.457451</td>\n",
       "      <td>0.496973</td>\n",
       "      <td>0.537416</td>\n",
       "      <td>0.575507</td>\n",
       "      <td>0.616503</td>\n",
       "      <td>0.658605</td>\n",
       "      <td>0.701921</td>\n",
       "      <td>0.746277</td>\n",
       "      <td>0.791765</td>\n",
       "      <td>...</td>\n",
       "      <td>4.100028</td>\n",
       "      <td>4.163747</td>\n",
       "      <td>4.216513</td>\n",
       "      <td>4.255809</td>\n",
       "      <td>4.280443</td>\n",
       "      <td>4.280814</td>\n",
       "      <td>4.237858</td>\n",
       "      <td>4.132937</td>\n",
       "      <td>3.978863</td>\n",
       "      <td>3.836942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0.465971</td>\n",
       "      <td>0.535950</td>\n",
       "      <td>0.599500</td>\n",
       "      <td>0.657984</td>\n",
       "      <td>0.709998</td>\n",
       "      <td>0.764922</td>\n",
       "      <td>0.820523</td>\n",
       "      <td>0.876953</td>\n",
       "      <td>0.934031</td>\n",
       "      <td>0.991913</td>\n",
       "      <td>...</td>\n",
       "      <td>4.857763</td>\n",
       "      <td>4.927424</td>\n",
       "      <td>4.984310</td>\n",
       "      <td>5.025386</td>\n",
       "      <td>5.049202</td>\n",
       "      <td>5.044452</td>\n",
       "      <td>4.988772</td>\n",
       "      <td>4.860395</td>\n",
       "      <td>4.674584</td>\n",
       "      <td>4.503457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.383691</td>\n",
       "      <td>0.450965</td>\n",
       "      <td>0.514631</td>\n",
       "      <td>0.572481</td>\n",
       "      <td>0.623304</td>\n",
       "      <td>0.676948</td>\n",
       "      <td>0.731329</td>\n",
       "      <td>0.786481</td>\n",
       "      <td>0.842163</td>\n",
       "      <td>0.898461</td>\n",
       "      <td>...</td>\n",
       "      <td>4.744066</td>\n",
       "      <td>4.813846</td>\n",
       "      <td>4.870592</td>\n",
       "      <td>4.911579</td>\n",
       "      <td>4.935582</td>\n",
       "      <td>4.931638</td>\n",
       "      <td>4.877883</td>\n",
       "      <td>4.753013</td>\n",
       "      <td>4.571928</td>\n",
       "      <td>4.405149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>0.202562</td>\n",
       "      <td>0.201751</td>\n",
       "      <td>0.203200</td>\n",
       "      <td>0.210987</td>\n",
       "      <td>0.221130</td>\n",
       "      <td>0.232812</td>\n",
       "      <td>0.245369</td>\n",
       "      <td>0.258871</td>\n",
       "      <td>0.273278</td>\n",
       "      <td>0.288644</td>\n",
       "      <td>...</td>\n",
       "      <td>1.673267</td>\n",
       "      <td>1.706698</td>\n",
       "      <td>1.737154</td>\n",
       "      <td>1.762956</td>\n",
       "      <td>1.783029</td>\n",
       "      <td>1.793023</td>\n",
       "      <td>1.784698</td>\n",
       "      <td>1.749866</td>\n",
       "      <td>1.693567</td>\n",
       "      <td>1.641708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>0.289292</td>\n",
       "      <td>0.303189</td>\n",
       "      <td>0.315865</td>\n",
       "      <td>0.332897</td>\n",
       "      <td>0.350998</td>\n",
       "      <td>0.370787</td>\n",
       "      <td>0.391281</td>\n",
       "      <td>0.412664</td>\n",
       "      <td>0.434948</td>\n",
       "      <td>0.458279</td>\n",
       "      <td>...</td>\n",
       "      <td>2.185162</td>\n",
       "      <td>2.223102</td>\n",
       "      <td>2.258088</td>\n",
       "      <td>2.287536</td>\n",
       "      <td>2.309717</td>\n",
       "      <td>2.318868</td>\n",
       "      <td>2.304381</td>\n",
       "      <td>2.255814</td>\n",
       "      <td>2.179809</td>\n",
       "      <td>2.109786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>0.381550</td>\n",
       "      <td>0.470170</td>\n",
       "      <td>0.548909</td>\n",
       "      <td>0.615927</td>\n",
       "      <td>0.673070</td>\n",
       "      <td>0.732666</td>\n",
       "      <td>0.792570</td>\n",
       "      <td>0.852905</td>\n",
       "      <td>0.913487</td>\n",
       "      <td>0.974471</td>\n",
       "      <td>...</td>\n",
       "      <td>5.003251</td>\n",
       "      <td>5.074765</td>\n",
       "      <td>5.132710</td>\n",
       "      <td>5.174146</td>\n",
       "      <td>5.197733</td>\n",
       "      <td>5.191909</td>\n",
       "      <td>5.133690</td>\n",
       "      <td>5.000708</td>\n",
       "      <td>4.808701</td>\n",
       "      <td>4.631874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1361</th>\n",
       "      <td>0.453996</td>\n",
       "      <td>0.544847</td>\n",
       "      <td>0.620312</td>\n",
       "      <td>0.684716</td>\n",
       "      <td>0.740257</td>\n",
       "      <td>0.798081</td>\n",
       "      <td>0.856012</td>\n",
       "      <td>0.914347</td>\n",
       "      <td>0.973024</td>\n",
       "      <td>1.032310</td>\n",
       "      <td>...</td>\n",
       "      <td>4.783330</td>\n",
       "      <td>4.849700</td>\n",
       "      <td>4.904975</td>\n",
       "      <td>4.945541</td>\n",
       "      <td>4.969423</td>\n",
       "      <td>4.965237</td>\n",
       "      <td>4.910910</td>\n",
       "      <td>4.784997</td>\n",
       "      <td>4.602506</td>\n",
       "      <td>4.434433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547</th>\n",
       "      <td>0.325858</td>\n",
       "      <td>0.330976</td>\n",
       "      <td>0.332963</td>\n",
       "      <td>0.341853</td>\n",
       "      <td>0.353758</td>\n",
       "      <td>0.367173</td>\n",
       "      <td>0.381218</td>\n",
       "      <td>0.396176</td>\n",
       "      <td>0.412134</td>\n",
       "      <td>0.429284</td>\n",
       "      <td>...</td>\n",
       "      <td>1.653589</td>\n",
       "      <td>1.682302</td>\n",
       "      <td>1.710493</td>\n",
       "      <td>1.735484</td>\n",
       "      <td>1.755333</td>\n",
       "      <td>1.765342</td>\n",
       "      <td>1.757317</td>\n",
       "      <td>1.723186</td>\n",
       "      <td>1.667904</td>\n",
       "      <td>1.616983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>0.382323</td>\n",
       "      <td>0.443196</td>\n",
       "      <td>0.497557</td>\n",
       "      <td>0.547403</td>\n",
       "      <td>0.591868</td>\n",
       "      <td>0.638816</td>\n",
       "      <td>0.686397</td>\n",
       "      <td>0.734750</td>\n",
       "      <td>0.783736</td>\n",
       "      <td>0.833505</td>\n",
       "      <td>...</td>\n",
       "      <td>4.197061</td>\n",
       "      <td>4.259449</td>\n",
       "      <td>4.311814</td>\n",
       "      <td>4.351090</td>\n",
       "      <td>4.375623</td>\n",
       "      <td>4.375401</td>\n",
       "      <td>4.330910</td>\n",
       "      <td>4.223122</td>\n",
       "      <td>4.065150</td>\n",
       "      <td>3.919641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1540 rows Ã— 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0.000005  0.000005  0.000005  0.000005  0.000005  0.000005  0.000005  \\\n",
       "805   0.532133  0.638418  0.726998  0.802254  0.866701  0.933640  1.000501   \n",
       "1798  0.414041  0.457451  0.496973  0.537416  0.575507  0.616503  0.658605   \n",
       "193   0.465971  0.535950  0.599500  0.657984  0.709998  0.764922  0.820523   \n",
       "162   0.383691  0.450965  0.514631  0.572481  0.623304  0.676948  0.731329   \n",
       "1211  0.202562  0.201751  0.203200  0.210987  0.221130  0.232812  0.245369   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "599   0.289292  0.303189  0.315865  0.332897  0.350998  0.370787  0.391281   \n",
       "1599  0.381550  0.470170  0.548909  0.615927  0.673070  0.732666  0.792570   \n",
       "1361  0.453996  0.544847  0.620312  0.684716  0.740257  0.798081  0.856012   \n",
       "1547  0.325858  0.330976  0.332963  0.341853  0.353758  0.367173  0.381218   \n",
       "863   0.382323  0.443196  0.497557  0.547403  0.591868  0.638816  0.686397   \n",
       "\n",
       "      0.000005  0.000005  0.000005  ...  0.000008  0.000008  0.000008  \\\n",
       "805   1.067615  1.134905  1.202684  ...  5.303224  5.372908  5.430458   \n",
       "1798  0.701921  0.746277  0.791765  ...  4.100028  4.163747  4.216513   \n",
       "193   0.876953  0.934031  0.991913  ...  4.857763  4.927424  4.984310   \n",
       "162   0.786481  0.842163  0.898461  ...  4.744066  4.813846  4.870592   \n",
       "1211  0.258871  0.273278  0.288644  ...  1.673267  1.706698  1.737154   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "599   0.412664  0.434948  0.458279  ...  2.185162  2.223102  2.258088   \n",
       "1599  0.852905  0.913487  0.974471  ...  5.003251  5.074765  5.132710   \n",
       "1361  0.914347  0.973024  1.032310  ...  4.783330  4.849700  4.904975   \n",
       "1547  0.396176  0.412134  0.429284  ...  1.653589  1.682302  1.710493   \n",
       "863   0.734750  0.783736  0.833505  ...  4.197061  4.259449  4.311814   \n",
       "\n",
       "      0.000008  0.000008  0.000008  0.000008  0.000008  0.000008  0.000008  \n",
       "805   5.471816  5.494759  5.486708  5.423339  5.281082  5.076630  4.888354  \n",
       "1798  4.255809  4.280443  4.280814  4.237858  4.132937  3.978863  3.836942  \n",
       "193   5.025386  5.049202  5.044452  4.988772  4.860395  4.674584  4.503457  \n",
       "162   4.911579  4.935582  4.931638  4.877883  4.753013  4.571928  4.405149  \n",
       "1211  1.762956  1.783029  1.793023  1.784698  1.749866  1.693567  1.641708  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "599   2.287536  2.309717  2.318868  2.304381  2.255814  2.179809  2.109786  \n",
       "1599  5.174146  5.197733  5.191909  5.133690  5.000708  4.808701  4.631874  \n",
       "1361  4.945541  4.969423  4.965237  4.910910  4.784997  4.602506  4.434433  \n",
       "1547  1.735484  1.755333  1.765342  1.757317  1.723186  1.667904  1.616983  \n",
       "863   4.351090  4.375623  4.375401  4.330910  4.223122  4.065150  3.919641  \n",
       "\n",
       "[1540 rows x 66 columns]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.lin1 = nn.Linear(input_size, 45)\n",
    "        self.lin2 = nn.Linear(45, 60)\n",
    "        self.lin3 = nn.Linear(60, 75)\n",
    "        self.lin4 = nn.Linear(75, 60)\n",
    "        self.lin_fin = nn.Linear(60, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.lin1(x))\n",
    "        x = F.leaky_relu(self.lin2(x))\n",
    "        x = F.leaky_relu(self.lin3(x))\n",
    "        x = F.leaky_relu(self.lin4(x))\n",
    "        x = self.lin_fin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = len(X_train)\n",
    "input_size = 66\n",
    "output_size = 1\n",
    "\n",
    "# store models in descending order (11, 10, 9...)\n",
    "models = []\n",
    "for i in range(11):\n",
    "    models.append(Net(input_size+i, output_size))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = args['lr']\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 11\n",
      "best loss: 3.2517004013061523 in epoch 1000\n",
      "\n",
      "Layer 10\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1540x66 and 67x45)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[271], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     17\u001b[0m \u001b[39m# forward pass\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m pred \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     20\u001b[0m \u001b[39m# loss\u001b[39;00m\n\u001b[0;32m     21\u001b[0m loss \u001b[39m=\u001b[39m criterion(pred, outputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[268], line 12\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 12\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mleaky_relu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlin1(x))\n\u001b[0;32m     13\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mleaky_relu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin2(x))\n\u001b[0;32m     14\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mleaky_relu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin3(x))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1540x66 and 67x45)"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    best_loss = np.inf\n",
    "    best_epoch = 0\n",
    "\n",
    "    print(f'Layer {11-i}')\n",
    "\n",
    "    optimizer = torch.optim.Adam(models[i].parameters(), lr=learning_rate)\n",
    "    model = models[i]\n",
    "\n",
    "    inputs = df_to_tensor(X_train)\n",
    "    outputs = df_to_tensor(y_train.iloc[:,10-i]).reshape(-1,1)\n",
    "\n",
    "    for epoch in range(args['epochs']):\n",
    "        # empty gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        pred = model(inputs)\n",
    "\n",
    "        # loss\n",
    "        loss = criterion(pred, outputs)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # update\n",
    "        optimizer.step()\n",
    "\n",
    "        #if (epoch+1) % int(args['epochs']/10) == 0:\n",
    "        #    print(f'epoch: {epoch+1}, loss = {loss}')\n",
    "\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_epoch = epoch + 1\n",
    "\n",
    "    pred_np = pred.detach().numpy()\n",
    "    #print(pred.detach().numpy())\n",
    "    #X_train['layer 11 predictions'] = pd.DataFrame(pred.detach().numpy())\n",
    "    print(f'best loss: {best_loss} in epoch {best_epoch}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
