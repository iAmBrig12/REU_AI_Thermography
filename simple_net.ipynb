{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'lr':0.01,\n",
    "        'train epochs':1500,\n",
    "        'hidden layers':6,\n",
    "        'noise':0.01,\n",
    "        'train size':0.7,\n",
    "        'spec scale':10**12,\n",
    "        'train criterion':nn.L1Loss(),\n",
    "        'test criterion':nn.L1Loss(),\n",
    "        'scaler':Normalizer(),\n",
    "        'dropout':0.5\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('data_3nm.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectrum data\n",
    "X = df.iloc[:,11:]\n",
    "\n",
    "# layer data\n",
    "y = df.iloc[:,:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale by 10^12\n",
    "X = X.apply(lambda x: x*args['spec scale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(data, percentage):\n",
    "    std_dev = percentage\n",
    "    noise = np.random.normal(0, std_dev, data.shape)\n",
    "    noisy_data = data + noise\n",
    "    return noisy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add noise\n",
    "X_noisy = add_noise(X, args['noise'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = args['scaler']\n",
    "scaler.fit(X_noisy)\n",
    "X_noisy_scaled = scaler.transform(X_noisy)\n",
    "\n",
    "X_noisy_scaled_df = pd.DataFrame(X_noisy_scaled, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_noisy_scaled_df, y, train_size=args['train size'], random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_hidden):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        hidden_sizes = [input_size]\n",
    "        step = input_size / (num_hidden+1)\n",
    "\n",
    "        for i in range(num_hidden):\n",
    "            hidden_size = int(input_size - (i+1) * step)\n",
    "            if hidden_size < 1: hidden_size = 1\n",
    "            hidden_sizes.append(hidden_size)\n",
    "        \n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_hidden):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.lin_fin = nn.Linear(hidden_sizes[-1], output_size)\n",
    "        self.dropout = nn.Dropout(args['dropout'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.leaky_relu(hidden_layer(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.lin_fin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = args['train criterion']\n",
    "model = Net(X_train_tensor.size()[1], y_train_tensor.size()[1], args['hidden layers'])\n",
    "optimizer = torch.optim.Rprop(model.parameters(), lr=args['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 150, loss = 23.917015075683594\n",
      "epoch: 300, loss = 18.657102584838867\n",
      "epoch: 450, loss = 16.632810592651367\n",
      "epoch: 600, loss = 15.21753215789795\n",
      "epoch: 750, loss = 14.892919540405273\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     14\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> 15\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     17\u001b[0m train_losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     19\u001b[0m \u001b[39mif\u001b[39;00m loss\u001b[39m.\u001b[39mitem() \u001b[39m<\u001b[39m best_loss:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\rprop.py:106\u001b[0m, in \u001b[0;36mRprop.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    102\u001b[0m     maximize \u001b[39m=\u001b[39m group[\u001b[39m\"\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    104\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(group, params, grads, prevs, step_sizes)\n\u001b[1;32m--> 106\u001b[0m     rprop(\n\u001b[0;32m    107\u001b[0m         params,\n\u001b[0;32m    108\u001b[0m         grads,\n\u001b[0;32m    109\u001b[0m         prevs,\n\u001b[0;32m    110\u001b[0m         step_sizes,\n\u001b[0;32m    111\u001b[0m         step_size_min\u001b[39m=\u001b[39;49mstep_size_min,\n\u001b[0;32m    112\u001b[0m         step_size_max\u001b[39m=\u001b[39;49mstep_size_max,\n\u001b[0;32m    113\u001b[0m         etaminus\u001b[39m=\u001b[39;49metaminus,\n\u001b[0;32m    114\u001b[0m         etaplus\u001b[39m=\u001b[39;49metaplus,\n\u001b[0;32m    115\u001b[0m         foreach\u001b[39m=\u001b[39;49mforeach,\n\u001b[0;32m    116\u001b[0m         maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    117\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    118\u001b[0m     )\n\u001b[0;32m    120\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\rprop.py:205\u001b[0m, in \u001b[0;36mrprop\u001b[1;34m(params, grads, prevs, step_sizes, foreach, maximize, differentiable, step_size_min, step_size_max, etaminus, etaplus)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_rprop\n\u001b[1;32m--> 205\u001b[0m func(\n\u001b[0;32m    206\u001b[0m     params,\n\u001b[0;32m    207\u001b[0m     grads,\n\u001b[0;32m    208\u001b[0m     prevs,\n\u001b[0;32m    209\u001b[0m     step_sizes,\n\u001b[0;32m    210\u001b[0m     step_size_min\u001b[39m=\u001b[39;49mstep_size_min,\n\u001b[0;32m    211\u001b[0m     step_size_max\u001b[39m=\u001b[39;49mstep_size_max,\n\u001b[0;32m    212\u001b[0m     etaminus\u001b[39m=\u001b[39;49metaminus,\n\u001b[0;32m    213\u001b[0m     etaplus\u001b[39m=\u001b[39;49metaplus,\n\u001b[0;32m    214\u001b[0m     maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    215\u001b[0m     differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    216\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\rprop.py:247\u001b[0m, in \u001b[0;36m_single_tensor_rprop\u001b[1;34m(params, grads, prevs, step_sizes, step_size_min, step_size_max, etaminus, etaplus, maximize, differentiable)\u001b[0m\n\u001b[0;32m    245\u001b[0m     sign \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39mmul(prev\u001b[39m.\u001b[39mclone())\u001b[39m.\u001b[39msign()\n\u001b[0;32m    246\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 247\u001b[0m     sign \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39;49mmul(prev)\u001b[39m.\u001b[39msign()\n\u001b[0;32m    248\u001b[0m sign[sign\u001b[39m.\u001b[39mgt(\u001b[39m0\u001b[39m)] \u001b[39m=\u001b[39m etaplus\n\u001b[0;32m    249\u001b[0m sign[sign\u001b[39m.\u001b[39mlt(\u001b[39m0\u001b[39m)] \u001b[39m=\u001b[39m etaminus\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_loss = np.inf\n",
    "best_epoch = 0\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(args['train epochs']):\n",
    "    # forward pass\n",
    "    outputs = model(X_train_tensor)\n",
    "\n",
    "    # get loss\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "    # update and backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        best_epoch = epoch+1\n",
    "\n",
    "    if (epoch+1) % int(args['train epochs']/10) == 0:\n",
    "        print(f'epoch: {epoch+1}, loss = {loss}')\n",
    "\n",
    "print(f'best loss: {best_loss} in epoch {best_epoch}\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.946070671081543\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    pred = model(X_test_tensor)\n",
    "    loss = criterion(pred, y_test_tensor)\n",
    "\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
