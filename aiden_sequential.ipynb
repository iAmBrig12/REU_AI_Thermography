{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from thermography_dataset_one_layer import ThermDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'lr':0.01,\n",
    "        'epochs':1000,\n",
    "        'noise':0,\n",
    "        'train size':0.7,\n",
    "        'spec scale':10**12\n",
    "        }\n",
    "\n",
    "num_layers = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_tensor(df):\n",
    "    return torch.tensor(df.values, dtype=torch.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('wide_range.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,11:]\n",
    "y = df.iloc[:,:11]\n",
    "\n",
    "X = X.apply(lambda x: x*args['spec scale'])\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=args['train size'], random_state=101)\n",
    "\n",
    "\n",
    "X_train.index = range(len(X_train))\n",
    "X_test.index = range(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.000004</th>\n",
       "      <th>0.000004</th>\n",
       "      <th>0.000004</th>\n",
       "      <th>0.000004</th>\n",
       "      <th>0.000004</th>\n",
       "      <th>0.000004</th>\n",
       "      <th>0.000004</th>\n",
       "      <th>0.000004</th>\n",
       "      <th>0.000004</th>\n",
       "      <th>0.000004</th>\n",
       "      <th>...</th>\n",
       "      <th>0.000008</th>\n",
       "      <th>0.000008</th>\n",
       "      <th>0.000008</th>\n",
       "      <th>0.000008</th>\n",
       "      <th>0.000008</th>\n",
       "      <th>0.000008</th>\n",
       "      <th>0.000008</th>\n",
       "      <th>0.000008</th>\n",
       "      <th>0.000008</th>\n",
       "      <th>0.000008</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.047328</td>\n",
       "      <td>0.044030</td>\n",
       "      <td>0.062621</td>\n",
       "      <td>0.050627</td>\n",
       "      <td>0.071193</td>\n",
       "      <td>0.068130</td>\n",
       "      <td>0.101914</td>\n",
       "      <td>0.075729</td>\n",
       "      <td>0.108325</td>\n",
       "      <td>0.133987</td>\n",
       "      <td>...</td>\n",
       "      <td>3.652357</td>\n",
       "      <td>3.706860</td>\n",
       "      <td>3.754378</td>\n",
       "      <td>3.791491</td>\n",
       "      <td>3.816109</td>\n",
       "      <td>3.819172</td>\n",
       "      <td>3.783524</td>\n",
       "      <td>3.692427</td>\n",
       "      <td>3.557224</td>\n",
       "      <td>3.432674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.038770</td>\n",
       "      <td>0.035673</td>\n",
       "      <td>0.050047</td>\n",
       "      <td>0.040308</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.054457</td>\n",
       "      <td>0.082382</td>\n",
       "      <td>0.062676</td>\n",
       "      <td>0.091149</td>\n",
       "      <td>0.114388</td>\n",
       "      <td>...</td>\n",
       "      <td>2.320425</td>\n",
       "      <td>2.355479</td>\n",
       "      <td>2.389728</td>\n",
       "      <td>2.419447</td>\n",
       "      <td>2.441954</td>\n",
       "      <td>2.450765</td>\n",
       "      <td>2.434611</td>\n",
       "      <td>2.382484</td>\n",
       "      <td>2.301435</td>\n",
       "      <td>2.226764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.017471</td>\n",
       "      <td>0.015108</td>\n",
       "      <td>0.019534</td>\n",
       "      <td>0.015249</td>\n",
       "      <td>0.021160</td>\n",
       "      <td>0.020663</td>\n",
       "      <td>0.033139</td>\n",
       "      <td>0.028438</td>\n",
       "      <td>0.044417</td>\n",
       "      <td>0.059315</td>\n",
       "      <td>...</td>\n",
       "      <td>1.783106</td>\n",
       "      <td>1.818651</td>\n",
       "      <td>1.850644</td>\n",
       "      <td>1.877459</td>\n",
       "      <td>1.898089</td>\n",
       "      <td>1.907974</td>\n",
       "      <td>1.898374</td>\n",
       "      <td>1.860607</td>\n",
       "      <td>1.800060</td>\n",
       "      <td>1.744286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.024575</td>\n",
       "      <td>0.020517</td>\n",
       "      <td>0.025340</td>\n",
       "      <td>0.019283</td>\n",
       "      <td>0.026321</td>\n",
       "      <td>0.025631</td>\n",
       "      <td>0.041952</td>\n",
       "      <td>0.037789</td>\n",
       "      <td>0.060454</td>\n",
       "      <td>0.082459</td>\n",
       "      <td>...</td>\n",
       "      <td>4.617065</td>\n",
       "      <td>4.680924</td>\n",
       "      <td>4.734778</td>\n",
       "      <td>4.774832</td>\n",
       "      <td>4.798921</td>\n",
       "      <td>4.795918</td>\n",
       "      <td>4.744459</td>\n",
       "      <td>4.623791</td>\n",
       "      <td>4.448374</td>\n",
       "      <td>4.286812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.012262</td>\n",
       "      <td>0.010760</td>\n",
       "      <td>0.014171</td>\n",
       "      <td>0.011194</td>\n",
       "      <td>0.015674</td>\n",
       "      <td>0.015384</td>\n",
       "      <td>0.024641</td>\n",
       "      <td>0.020957</td>\n",
       "      <td>0.032692</td>\n",
       "      <td>0.043657</td>\n",
       "      <td>...</td>\n",
       "      <td>1.639148</td>\n",
       "      <td>1.670154</td>\n",
       "      <td>1.699314</td>\n",
       "      <td>1.724537</td>\n",
       "      <td>1.744379</td>\n",
       "      <td>1.754398</td>\n",
       "      <td>1.746490</td>\n",
       "      <td>1.712636</td>\n",
       "      <td>1.657755</td>\n",
       "      <td>1.607205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1535</th>\n",
       "      <td>0.008816</td>\n",
       "      <td>0.007552</td>\n",
       "      <td>0.009605</td>\n",
       "      <td>0.007467</td>\n",
       "      <td>0.010369</td>\n",
       "      <td>0.010237</td>\n",
       "      <td>0.016819</td>\n",
       "      <td>0.015040</td>\n",
       "      <td>0.024138</td>\n",
       "      <td>0.032982</td>\n",
       "      <td>...</td>\n",
       "      <td>1.737692</td>\n",
       "      <td>1.768599</td>\n",
       "      <td>1.798230</td>\n",
       "      <td>1.824086</td>\n",
       "      <td>1.844388</td>\n",
       "      <td>1.854326</td>\n",
       "      <td>1.845326</td>\n",
       "      <td>1.808934</td>\n",
       "      <td>1.750374</td>\n",
       "      <td>1.696432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>0.022660</td>\n",
       "      <td>0.019827</td>\n",
       "      <td>0.026071</td>\n",
       "      <td>0.020487</td>\n",
       "      <td>0.028498</td>\n",
       "      <td>0.027720</td>\n",
       "      <td>0.043868</td>\n",
       "      <td>0.036680</td>\n",
       "      <td>0.056381</td>\n",
       "      <td>0.074280</td>\n",
       "      <td>...</td>\n",
       "      <td>1.875067</td>\n",
       "      <td>1.911222</td>\n",
       "      <td>1.943955</td>\n",
       "      <td>1.971422</td>\n",
       "      <td>1.992454</td>\n",
       "      <td>2.002216</td>\n",
       "      <td>1.991539</td>\n",
       "      <td>1.951336</td>\n",
       "      <td>1.887281</td>\n",
       "      <td>1.828272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>0.054368</td>\n",
       "      <td>0.049658</td>\n",
       "      <td>0.069249</td>\n",
       "      <td>0.055489</td>\n",
       "      <td>0.077627</td>\n",
       "      <td>0.074266</td>\n",
       "      <td>0.111904</td>\n",
       "      <td>0.084833</td>\n",
       "      <td>0.122915</td>\n",
       "      <td>0.153867</td>\n",
       "      <td>...</td>\n",
       "      <td>5.344826</td>\n",
       "      <td>5.409761</td>\n",
       "      <td>5.465227</td>\n",
       "      <td>5.505966</td>\n",
       "      <td>5.528740</td>\n",
       "      <td>5.520424</td>\n",
       "      <td>5.456460</td>\n",
       "      <td>5.313137</td>\n",
       "      <td>5.107256</td>\n",
       "      <td>4.917666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1538</th>\n",
       "      <td>0.017072</td>\n",
       "      <td>0.014896</td>\n",
       "      <td>0.019520</td>\n",
       "      <td>0.015351</td>\n",
       "      <td>0.021409</td>\n",
       "      <td>0.020917</td>\n",
       "      <td>0.033378</td>\n",
       "      <td>0.028303</td>\n",
       "      <td>0.043981</td>\n",
       "      <td>0.058593</td>\n",
       "      <td>...</td>\n",
       "      <td>2.570995</td>\n",
       "      <td>2.615463</td>\n",
       "      <td>2.655006</td>\n",
       "      <td>2.687193</td>\n",
       "      <td>2.710532</td>\n",
       "      <td>2.718533</td>\n",
       "      <td>2.698864</td>\n",
       "      <td>2.639391</td>\n",
       "      <td>2.547993</td>\n",
       "      <td>2.463787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>0.021359</td>\n",
       "      <td>0.019676</td>\n",
       "      <td>0.027645</td>\n",
       "      <td>0.022377</td>\n",
       "      <td>0.031646</td>\n",
       "      <td>0.030620</td>\n",
       "      <td>0.046760</td>\n",
       "      <td>0.036060</td>\n",
       "      <td>0.052949</td>\n",
       "      <td>0.067222</td>\n",
       "      <td>...</td>\n",
       "      <td>3.392286</td>\n",
       "      <td>3.448372</td>\n",
       "      <td>3.495906</td>\n",
       "      <td>3.532569</td>\n",
       "      <td>3.557166</td>\n",
       "      <td>3.561590</td>\n",
       "      <td>3.529876</td>\n",
       "      <td>3.446362</td>\n",
       "      <td>3.321571</td>\n",
       "      <td>3.206608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1540 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0.000004  0.000004  0.000004  0.000004  0.000004  0.000004  0.000004  \\\n",
       "0     0.047328  0.044030  0.062621  0.050627  0.071193  0.068130  0.101914   \n",
       "1     0.038770  0.035673  0.050047  0.040308  0.056641  0.054457  0.082382   \n",
       "2     0.017471  0.015108  0.019534  0.015249  0.021160  0.020663  0.033139   \n",
       "3     0.024575  0.020517  0.025340  0.019283  0.026321  0.025631  0.041952   \n",
       "4     0.012262  0.010760  0.014171  0.011194  0.015674  0.015384  0.024641   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1535  0.008816  0.007552  0.009605  0.007467  0.010369  0.010237  0.016819   \n",
       "1536  0.022660  0.019827  0.026071  0.020487  0.028498  0.027720  0.043868   \n",
       "1537  0.054368  0.049658  0.069249  0.055489  0.077627  0.074266  0.111904   \n",
       "1538  0.017072  0.014896  0.019520  0.015351  0.021409  0.020917  0.033378   \n",
       "1539  0.021359  0.019676  0.027645  0.022377  0.031646  0.030620  0.046760   \n",
       "\n",
       "      0.000004  0.000004  0.000004  ...  0.000008  0.000008  0.000008  \\\n",
       "0     0.075729  0.108325  0.133987  ...  3.652357  3.706860  3.754378   \n",
       "1     0.062676  0.091149  0.114388  ...  2.320425  2.355479  2.389728   \n",
       "2     0.028438  0.044417  0.059315  ...  1.783106  1.818651  1.850644   \n",
       "3     0.037789  0.060454  0.082459  ...  4.617065  4.680924  4.734778   \n",
       "4     0.020957  0.032692  0.043657  ...  1.639148  1.670154  1.699314   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1535  0.015040  0.024138  0.032982  ...  1.737692  1.768599  1.798230   \n",
       "1536  0.036680  0.056381  0.074280  ...  1.875067  1.911222  1.943955   \n",
       "1537  0.084833  0.122915  0.153867  ...  5.344826  5.409761  5.465227   \n",
       "1538  0.028303  0.043981  0.058593  ...  2.570995  2.615463  2.655006   \n",
       "1539  0.036060  0.052949  0.067222  ...  3.392286  3.448372  3.495906   \n",
       "\n",
       "      0.000008  0.000008  0.000008  0.000008  0.000008  0.000008  0.000008  \n",
       "0     3.791491  3.816109  3.819172  3.783524  3.692427  3.557224  3.432674  \n",
       "1     2.419447  2.441954  2.450765  2.434611  2.382484  2.301435  2.226764  \n",
       "2     1.877459  1.898089  1.907974  1.898374  1.860607  1.800060  1.744286  \n",
       "3     4.774832  4.798921  4.795918  4.744459  4.623791  4.448374  4.286812  \n",
       "4     1.724537  1.744379  1.754398  1.746490  1.712636  1.657755  1.607205  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1535  1.824086  1.844388  1.854326  1.845326  1.808934  1.750374  1.696432  \n",
       "1536  1.971422  1.992454  2.002216  1.991539  1.951336  1.887281  1.828272  \n",
       "1537  5.505966  5.528740  5.520424  5.456460  5.313137  5.107256  4.917666  \n",
       "1538  2.687193  2.710532  2.718533  2.698864  2.639391  2.547993  2.463787  \n",
       "1539  3.532569  3.557166  3.561590  3.529876  3.446362  3.321571  3.206608  \n",
       "\n",
       "[1540 rows x 86 columns]"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer 1</th>\n",
       "      <th>layer 2</th>\n",
       "      <th>layer 3</th>\n",
       "      <th>layer 4</th>\n",
       "      <th>layer 5</th>\n",
       "      <th>layer 6</th>\n",
       "      <th>layer 7</th>\n",
       "      <th>layer 8</th>\n",
       "      <th>layer 9</th>\n",
       "      <th>layer 10</th>\n",
       "      <th>layer 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>370.341309</td>\n",
       "      <td>328.561339</td>\n",
       "      <td>326.246196</td>\n",
       "      <td>325.960978</td>\n",
       "      <td>337.115108</td>\n",
       "      <td>338.011684</td>\n",
       "      <td>329.244973</td>\n",
       "      <td>338.854172</td>\n",
       "      <td>334.003205</td>\n",
       "      <td>331.473263</td>\n",
       "      <td>344.660386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>358.717780</td>\n",
       "      <td>321.142036</td>\n",
       "      <td>335.121747</td>\n",
       "      <td>323.227587</td>\n",
       "      <td>334.855686</td>\n",
       "      <td>331.980768</td>\n",
       "      <td>331.899256</td>\n",
       "      <td>332.369302</td>\n",
       "      <td>336.197365</td>\n",
       "      <td>344.170862</td>\n",
       "      <td>318.387290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>303.578596</td>\n",
       "      <td>296.012106</td>\n",
       "      <td>320.139477</td>\n",
       "      <td>331.612634</td>\n",
       "      <td>343.149874</td>\n",
       "      <td>341.599287</td>\n",
       "      <td>327.050020</td>\n",
       "      <td>312.050438</td>\n",
       "      <td>300.909822</td>\n",
       "      <td>290.052707</td>\n",
       "      <td>305.241303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>281.108362</td>\n",
       "      <td>326.802138</td>\n",
       "      <td>327.622403</td>\n",
       "      <td>330.956774</td>\n",
       "      <td>328.315642</td>\n",
       "      <td>325.316098</td>\n",
       "      <td>330.881045</td>\n",
       "      <td>323.297827</td>\n",
       "      <td>335.718231</td>\n",
       "      <td>335.832163</td>\n",
       "      <td>359.882238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>300.756094</td>\n",
       "      <td>302.244654</td>\n",
       "      <td>302.319500</td>\n",
       "      <td>307.933527</td>\n",
       "      <td>316.877701</td>\n",
       "      <td>315.324771</td>\n",
       "      <td>318.254083</td>\n",
       "      <td>318.875332</td>\n",
       "      <td>313.324653</td>\n",
       "      <td>307.896394</td>\n",
       "      <td>301.072836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>274.142753</td>\n",
       "      <td>312.028924</td>\n",
       "      <td>305.203144</td>\n",
       "      <td>306.792005</td>\n",
       "      <td>301.311853</td>\n",
       "      <td>291.127893</td>\n",
       "      <td>298.271076</td>\n",
       "      <td>303.973224</td>\n",
       "      <td>308.318167</td>\n",
       "      <td>319.322458</td>\n",
       "      <td>303.811769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>318.512434</td>\n",
       "      <td>351.712729</td>\n",
       "      <td>343.852709</td>\n",
       "      <td>337.011663</td>\n",
       "      <td>325.408620</td>\n",
       "      <td>325.728092</td>\n",
       "      <td>318.063111</td>\n",
       "      <td>309.798561</td>\n",
       "      <td>314.474813</td>\n",
       "      <td>297.388722</td>\n",
       "      <td>307.688462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1361</th>\n",
       "      <td>370.224132</td>\n",
       "      <td>340.547318</td>\n",
       "      <td>330.792273</td>\n",
       "      <td>325.245505</td>\n",
       "      <td>318.753576</td>\n",
       "      <td>321.749459</td>\n",
       "      <td>328.052407</td>\n",
       "      <td>338.216759</td>\n",
       "      <td>343.711867</td>\n",
       "      <td>361.540581</td>\n",
       "      <td>369.965830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547</th>\n",
       "      <td>308.722907</td>\n",
       "      <td>320.029113</td>\n",
       "      <td>305.872919</td>\n",
       "      <td>303.670763</td>\n",
       "      <td>308.517581</td>\n",
       "      <td>320.283005</td>\n",
       "      <td>326.395634</td>\n",
       "      <td>328.204725</td>\n",
       "      <td>333.495631</td>\n",
       "      <td>311.017505</td>\n",
       "      <td>324.166657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>338.422663</td>\n",
       "      <td>306.055939</td>\n",
       "      <td>297.007469</td>\n",
       "      <td>294.145054</td>\n",
       "      <td>307.124023</td>\n",
       "      <td>320.066599</td>\n",
       "      <td>321.902875</td>\n",
       "      <td>319.890125</td>\n",
       "      <td>323.116906</td>\n",
       "      <td>302.077218</td>\n",
       "      <td>340.244495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1540 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         layer 1     layer 2     layer 3     layer 4     layer 5     layer 6  \\\n",
       "805   370.341309  328.561339  326.246196  325.960978  337.115108  338.011684   \n",
       "1798  358.717780  321.142036  335.121747  323.227587  334.855686  331.980768   \n",
       "193   303.578596  296.012106  320.139477  331.612634  343.149874  341.599287   \n",
       "162   281.108362  326.802138  327.622403  330.956774  328.315642  325.316098   \n",
       "1211  300.756094  302.244654  302.319500  307.933527  316.877701  315.324771   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "599   274.142753  312.028924  305.203144  306.792005  301.311853  291.127893   \n",
       "1599  318.512434  351.712729  343.852709  337.011663  325.408620  325.728092   \n",
       "1361  370.224132  340.547318  330.792273  325.245505  318.753576  321.749459   \n",
       "1547  308.722907  320.029113  305.872919  303.670763  308.517581  320.283005   \n",
       "863   338.422663  306.055939  297.007469  294.145054  307.124023  320.066599   \n",
       "\n",
       "         layer 7     layer 8     layer 9   layer 10     layer 11  \n",
       "805   329.244973  338.854172  334.003205  331.473263  344.660386  \n",
       "1798  331.899256  332.369302  336.197365  344.170862  318.387290  \n",
       "193   327.050020  312.050438  300.909822  290.052707  305.241303  \n",
       "162   330.881045  323.297827  335.718231  335.832163  359.882238  \n",
       "1211  318.254083  318.875332  313.324653  307.896394  301.072836  \n",
       "...          ...         ...         ...         ...         ...  \n",
       "599   298.271076  303.973224  308.318167  319.322458  303.811769  \n",
       "1599  318.063111  309.798561  314.474813  297.388722  307.688462  \n",
       "1361  328.052407  338.216759  343.711867  361.540581  369.965830  \n",
       "1547  326.395634  328.204725  333.495631  311.017505  324.166657  \n",
       "863   321.902875  319.890125  323.116906  302.077218  340.244495  \n",
       "\n",
       "[1540 rows x 11 columns]"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.lin1 = nn.Linear(input_size, 45)\n",
    "        self.lin2 = nn.Linear(45, 60)\n",
    "        self.lin3 = nn.Linear(60, 75)\n",
    "        self.lin4 = nn.Linear(75, 60)\n",
    "        self.lin_fin = nn.Linear(60, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.lin1(x))\n",
    "        x = F.leaky_relu(self.lin2(x))\n",
    "        x = F.leaky_relu(self.lin3(x))\n",
    "        x = F.leaky_relu(self.lin4(x))\n",
    "        x = self.lin_fin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " input size: 86, output size: 1\n",
      "{'model1': Net(\n",
      "  (lin1): Linear(in_features=96, out_features=45, bias=True)\n",
      "  (lin2): Linear(in_features=45, out_features=60, bias=True)\n",
      "  (lin3): Linear(in_features=60, out_features=75, bias=True)\n",
      "  (lin4): Linear(in_features=75, out_features=60, bias=True)\n",
      "  (lin_fin): Linear(in_features=60, out_features=1, bias=True)\n",
      "), 'model2': Net(\n",
      "  (lin1): Linear(in_features=95, out_features=45, bias=True)\n",
      "  (lin2): Linear(in_features=45, out_features=60, bias=True)\n",
      "  (lin3): Linear(in_features=60, out_features=75, bias=True)\n",
      "  (lin4): Linear(in_features=75, out_features=60, bias=True)\n",
      "  (lin_fin): Linear(in_features=60, out_features=1, bias=True)\n",
      "), 'model3': Net(\n",
      "  (lin1): Linear(in_features=94, out_features=45, bias=True)\n",
      "  (lin2): Linear(in_features=45, out_features=60, bias=True)\n",
      "  (lin3): Linear(in_features=60, out_features=75, bias=True)\n",
      "  (lin4): Linear(in_features=75, out_features=60, bias=True)\n",
      "  (lin_fin): Linear(in_features=60, out_features=1, bias=True)\n",
      "), 'model4': Net(\n",
      "  (lin1): Linear(in_features=93, out_features=45, bias=True)\n",
      "  (lin2): Linear(in_features=45, out_features=60, bias=True)\n",
      "  (lin3): Linear(in_features=60, out_features=75, bias=True)\n",
      "  (lin4): Linear(in_features=75, out_features=60, bias=True)\n",
      "  (lin_fin): Linear(in_features=60, out_features=1, bias=True)\n",
      "), 'model5': Net(\n",
      "  (lin1): Linear(in_features=92, out_features=45, bias=True)\n",
      "  (lin2): Linear(in_features=45, out_features=60, bias=True)\n",
      "  (lin3): Linear(in_features=60, out_features=75, bias=True)\n",
      "  (lin4): Linear(in_features=75, out_features=60, bias=True)\n",
      "  (lin_fin): Linear(in_features=60, out_features=1, bias=True)\n",
      "), 'model6': Net(\n",
      "  (lin1): Linear(in_features=91, out_features=45, bias=True)\n",
      "  (lin2): Linear(in_features=45, out_features=60, bias=True)\n",
      "  (lin3): Linear(in_features=60, out_features=75, bias=True)\n",
      "  (lin4): Linear(in_features=75, out_features=60, bias=True)\n",
      "  (lin_fin): Linear(in_features=60, out_features=1, bias=True)\n",
      "), 'model7': Net(\n",
      "  (lin1): Linear(in_features=90, out_features=45, bias=True)\n",
      "  (lin2): Linear(in_features=45, out_features=60, bias=True)\n",
      "  (lin3): Linear(in_features=60, out_features=75, bias=True)\n",
      "  (lin4): Linear(in_features=75, out_features=60, bias=True)\n",
      "  (lin_fin): Linear(in_features=60, out_features=1, bias=True)\n",
      "), 'model8': Net(\n",
      "  (lin1): Linear(in_features=89, out_features=45, bias=True)\n",
      "  (lin2): Linear(in_features=45, out_features=60, bias=True)\n",
      "  (lin3): Linear(in_features=60, out_features=75, bias=True)\n",
      "  (lin4): Linear(in_features=75, out_features=60, bias=True)\n",
      "  (lin_fin): Linear(in_features=60, out_features=1, bias=True)\n",
      "), 'model9': Net(\n",
      "  (lin1): Linear(in_features=88, out_features=45, bias=True)\n",
      "  (lin2): Linear(in_features=45, out_features=60, bias=True)\n",
      "  (lin3): Linear(in_features=60, out_features=75, bias=True)\n",
      "  (lin4): Linear(in_features=75, out_features=60, bias=True)\n",
      "  (lin_fin): Linear(in_features=60, out_features=1, bias=True)\n",
      "), 'model10': Net(\n",
      "  (lin1): Linear(in_features=87, out_features=45, bias=True)\n",
      "  (lin2): Linear(in_features=45, out_features=60, bias=True)\n",
      "  (lin3): Linear(in_features=60, out_features=75, bias=True)\n",
      "  (lin4): Linear(in_features=75, out_features=60, bias=True)\n",
      "  (lin_fin): Linear(in_features=60, out_features=1, bias=True)\n",
      "), 'model11': Net(\n",
      "  (lin1): Linear(in_features=86, out_features=45, bias=True)\n",
      "  (lin2): Linear(in_features=45, out_features=60, bias=True)\n",
      "  (lin3): Linear(in_features=60, out_features=75, bias=True)\n",
      "  (lin4): Linear(in_features=75, out_features=60, bias=True)\n",
      "  (lin_fin): Linear(in_features=60, out_features=1, bias=True)\n",
      ")}\n"
     ]
    }
   ],
   "source": [
    "input_size = len(X_train.columns)\n",
    "output_size = 1\n",
    "\n",
    "print(f' input size: {input_size}, output size: {output_size}')\n",
    "\n",
    "models = {}\n",
    "for i in range(1, 12):\n",
    "    models[f'model{i}'] = Net(input_size + (-i+11), output_size)\n",
    "\n",
    "print(models)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = args['lr']\n",
    "criterion = nn.L1Loss()\n",
    "num_epochs = args['epochs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 11\n",
      "tensor([[0.0473, 0.0440, 0.0626,  ..., 3.6924, 3.5572, 3.4327],\n",
      "        [0.0388, 0.0357, 0.0500,  ..., 2.3825, 2.3014, 2.2268],\n",
      "        [0.0175, 0.0151, 0.0195,  ..., 1.8606, 1.8001, 1.7443],\n",
      "        ...,\n",
      "        [0.0544, 0.0497, 0.0692,  ..., 5.3131, 5.1073, 4.9177],\n",
      "        [0.0171, 0.0149, 0.0195,  ..., 2.6394, 2.5480, 2.4638],\n",
      "        [0.0214, 0.0197, 0.0276,  ..., 3.4464, 3.3216, 3.2066]])\n",
      "inputs shape: torch.Size([1540, 86])\n",
      "model input shape: Net(\n",
      "  (lin1): Linear(in_features=86, out_features=45, bias=True)\n",
      "  (lin2): Linear(in_features=45, out_features=60, bias=True)\n",
      "  (lin3): Linear(in_features=60, out_features=75, bias=True)\n",
      "  (lin4): Linear(in_features=75, out_features=60, bias=True)\n",
      "  (lin_fin): Linear(in_features=60, out_features=1, bias=True)\n",
      ")\n",
      "epoch: 100, loss = 321.6754455566406\n",
      "epoch: 200, loss = 320.94586181640625\n",
      "epoch: 300, loss = 319.7139892578125\n",
      "epoch: 400, loss = 318.9426574707031\n",
      "epoch: 500, loss = 317.6376037597656\n",
      "epoch: 600, loss = 316.9668273925781\n",
      "epoch: 700, loss = 315.9573669433594\n",
      "epoch: 800, loss = 314.8670959472656\n",
      "epoch: 900, loss = 313.99566650390625\n",
      "epoch: 1000, loss = 312.9361267089844\n",
      "Layer 10\n",
      "tensor([[0.0473, 0.0440, 0.0626,  ..., 3.5572, 3.4327, 9.9174],\n",
      "        [0.0388, 0.0357, 0.0500,  ..., 2.3014, 2.2268, 9.9216],\n",
      "        [0.0175, 0.0151, 0.0195,  ..., 1.8001, 1.7443, 9.9260],\n",
      "        ...,\n",
      "        [0.0544, 0.0497, 0.0692,  ..., 5.1073, 4.9177, 9.9129],\n",
      "        [0.0171, 0.0149, 0.0195,  ..., 2.5480, 2.4638, 9.9212],\n",
      "        [0.0214, 0.0197, 0.0276,  ..., 3.3216, 3.2066, 9.9186]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "inputs shape: torch.Size([1540, 87])\n",
      "model input shape: Net(\n",
      "  (lin1): Linear(in_features=87, out_features=45, bias=True)\n",
      "  (lin2): Linear(in_features=45, out_features=60, bias=True)\n",
      "  (lin3): Linear(in_features=60, out_features=75, bias=True)\n",
      "  (lin4): Linear(in_features=75, out_features=60, bias=True)\n",
      "  (lin_fin): Linear(in_features=60, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aidenkarpf/anaconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:200: UserWarning: Error detected in AddmmBackward0. Traceback of forward call that caused the error:\n",
      "  File \"/Users/aidenkarpf/anaconda3/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Users/aidenkarpf/anaconda3/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/aidenkarpf/anaconda3/lib/python3.10/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/aidenkarpf/anaconda3/lib/python3.10/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/aidenkarpf/anaconda3/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 677, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/aidenkarpf/anaconda3/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 215, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/aidenkarpf/anaconda3/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/aidenkarpf/anaconda3/lib/python3.10/asyncio/base_events.py\", line 1906, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/aidenkarpf/anaconda3/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/aidenkarpf/anaconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/aidenkarpf/anaconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 460, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/aidenkarpf/anaconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 367, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/aidenkarpf/anaconda3/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 662, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/aidenkarpf/anaconda3/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 360, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/Users/aidenkarpf/anaconda3/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/aidenkarpf/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2880, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/aidenkarpf/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2935, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/Users/aidenkarpf/anaconda3/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/aidenkarpf/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3134, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/aidenkarpf/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3337, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/aidenkarpf/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3397, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/52/r731_9qx1795shmksbc8_c240000gn/T/ipykernel_33457/1068466316.py\", line 44, in <cell line: 7>\n",
      "    pred_temp = model(inputs)\n",
      "  File \"/Users/aidenkarpf/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/var/folders/52/r731_9qx1795shmksbc8_c240000gn/T/ipykernel_33457/4293958622.py\", line 16, in forward\n",
      "    x = self.lin_fin(x)\n",
      "  File \"/Users/aidenkarpf/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/aidenkarpf/anaconda3/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      " (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1682343686130/work/torch/csrc/autograd/python_anomaly_mode.cpp:119.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/aidenkarpf/Documents/GitHub/REU_AI_Thermography/aiden_sequential.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aidenkarpf/Documents/GitHub/REU_AI_Thermography/aiden_sequential.ipynb#X21sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m loss_lists[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mloss_list\u001b[39m\u001b[39m{\u001b[39;00mlayer\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aidenkarpf/Documents/GitHub/REU_AI_Thermography/aiden_sequential.ipynb#X21sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# backward pass\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/aidenkarpf/Documents/GitHub/REU_AI_Thermography/aiden_sequential.ipynb#X21sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aidenkarpf/Documents/GitHub/REU_AI_Thermography/aiden_sequential.ipynb#X21sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m# update\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aidenkarpf/Documents/GitHub/REU_AI_Thermography/aiden_sequential.ipynb#X21sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "loss_lists = {}\n",
    "losses = {}\n",
    "inputs = df_to_tensor(X_train)\n",
    "prev_pred = None\n",
    "\n",
    "\n",
    "for i in range(num_layers):\n",
    "    layer = 11 - i\n",
    "    print(f'Layer {layer}')\n",
    "\n",
    "    # define loss lists\n",
    "    loss_list = f'loss_list{layer}'\n",
    "    loss_lists[loss_list] = []\n",
    "\n",
    "    layer_loss_name = f'loss{layer}'\n",
    "    losses[layer_loss_name] = 0\n",
    "\n",
    "    # define the model corresponding to the given layer\n",
    "    model = models[f'model{layer}']\n",
    "\n",
    "    # define optimizer corresponding to the given model\n",
    "    optimizer = torch.optim.SGD(models[f'model{layer}'].parameters(), lr=learning_rate)\n",
    "\n",
    "    # add previous predictions to inputs if necessary\n",
    "\n",
    "    if i > 0:\n",
    "        pred_temp_new = pred_temp.clone()\n",
    "        prev_pred = pred_temp_new.reshape(-1, 1)\n",
    "        inputs = torch.cat([inputs, prev_pred], dim=1)\n",
    "\n",
    "    print(inputs)\n",
    "    print(f'inputs shape: {inputs.shape}')\n",
    "    print(f'model input shape: {model}')\n",
    "\n",
    "    # get expected_temp appropriately\n",
    "    expected_temp = df_to_tensor(y_train.iloc[:,layer-1]).reshape(-1,1)\n",
    "\n",
    "    # begin training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # empty gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        pred_temp = model(inputs)\n",
    "\n",
    "        # calculate loss and add to loss list\n",
    "        loss = criterion(pred_temp, expected_temp)\n",
    "        loss_lists[f'loss_list{layer}'].append(loss.item())\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # update\n",
    "        optimizer.step()\n",
    "\n",
    "        # print losses\n",
    "        if (epoch+1) % int(args['epochs']/10) == 0:\n",
    "            print(f'epoch: {epoch+1}, loss = {loss}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
